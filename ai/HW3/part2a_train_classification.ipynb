{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "I have implemented the test function for accuracy and loss. I experimented with BatchNorm2d and Dropout layers, and here are the results:\n",
    "\n",
    "\n",
    "#### Default model\n",
    "\n",
    "The accuracy is good- 97% on test set\n",
    "\n",
    "#### 1. BatchNorm2d\n",
    "\n",
    "With BatchNorm2d layer the training happened a lot faster. For example:\n",
    "\n",
    "Without BatchNorm2d layer after first epoch: train loss- 0.0181, test loss- 0.011\n",
    "\n",
    "With BatchNorm2d layer after first epoch:    train loss- 0.0127, test loss- 0.0076\n",
    "\n",
    "Also the accuracy with BatchNorm2d layer was a bit higher. 97.7%-- the accuracy for batchNorm if I place the batchnorm layer after first Conv layer\n",
    "\n",
    "\n",
    "#### 1. Dropout\n",
    "\n",
    "With Dropout regularization layer I managed to get worse results.\n",
    "\n",
    "Without Dropout layer, 20 epochs\n",
    "[0.9664, 0.975, 0.9754, 0.9816, 0.9794, 0.9808, 0.9799, 0.981, 0.9811, 0.9803, 0.9815, 0.9828, 0.9815, 0.9825, 0.9799, 0.9827, 0.9838, 0.9836, 0.983, 0.9781]\n",
    "\n",
    "With Dropout layer, 20 epochs\n",
    "[96.3, 96.2, 96.7, 96.6, 97.0, 96.8, 97.1, 97.0, 97.0, 97.1, 97.2, 97.0, 97.2, 97.4, 97.1, 97.1, 97.0, 96.6, 97.2]\n",
    "\n",
    "I assume that the problem is that our network is too shallow for Dropout here. Dropout somehow only damages the performance of the network. We can also see how our network start to overfit without the dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Liteweight network architecture for the Mnist dataset (digit) classification\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=False):\n",
    "        super(MnistNet, self).__init__()\n",
    "        self.num_classes = 10\n",
    "        \n",
    "        if dropout:\n",
    "            # fully convolutional part\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(1, 4, kernel_size=5),\n",
    "                nn.BatchNorm2d(4),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=0.1),\n",
    "                nn.Conv2d(4, 4, kernel_size=5),\n",
    "                nn.BatchNorm2d(4),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=0.1)\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            # fully convolutional part\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(1, 4, kernel_size=5),\n",
    "                nn.BatchNorm2d(4),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(4, 4, kernel_size=5),\n",
    "                nn.BatchNorm2d(4),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.ReLU(inplace=True)            \n",
    "            )\n",
    "        if dropout:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(16*4,16),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Linear(16,self.num_classes)\n",
    "            )\n",
    "        else:\n",
    "            # classifier, FC layers\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(16*4,16),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(16,self.num_classes)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x.view(-1,x.size(-3)*x.size(-2)*x.size(-1)))\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer):\n",
    "    \"\"\"\n",
    "    Training of an epoch\n",
    "    model: network\n",
    "    train_loader: train_loader loading images and labels in batches\n",
    "    optimizer: optimizer to use in the training\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() # zero the accumulated gradients\n",
    "        output = model(data) # computer network's output\n",
    "\n",
    "        loss = F.cross_entropy(output, target) # computer the loss\n",
    "\n",
    "\n",
    "        loss.backward() # backward pass\n",
    "        optimizer.step() # update weights\n",
    "        \n",
    "        total_loss = total_loss + loss.item()\n",
    "        \n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('[{}/{} ({:.0f}%)]\\tBatch loss: {:.6f}'.format(\n",
    "                batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()/len(data)))\n",
    "    \n",
    "    print('Training: Epoch average loss {:.6f}'.format(total_loss/len(train_loader.dataset)))\n",
    "         \n",
    "        \n",
    "def test(model, val_loader):\n",
    "    \"\"\"\n",
    "    Compute accuracy on the validation set\n",
    "    model: network\n",
    "    val_loader: test_loader loading images and labels in batches\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # implement validation procedure, report accuracy on the validation set\n",
    "    \n",
    "    batches = len(val_loader)\n",
    "    size = len(val_loader.dataset)\n",
    "    batches, size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_sum, accuracy = 0, 0\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            predictions = model(X)\n",
    "            loss = F.cross_entropy(predictions, y).item()\n",
    "            loss_sum += loss\n",
    "\n",
    "            accuracy += (predictions.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= batches\n",
    "    accuracy /= size\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m84200118\\Anaconda3\\envs\\v-env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:62: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mnist dataset structure - train part\n",
    "mnist_dataset_train = datasets.MNIST('vs3ex1data/mnist_data', train=True, download=True, transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ]))\n",
    "# mnist dataset structure - test part\n",
    "mnist_dataset_val = datasets.MNIST('vs3ex1data/mnist_data', train=False, transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ]))\n",
    "\n",
    "# show sample images\n",
    "print('Sample images')\n",
    "for i in range(0,100,100):\n",
    "    plt.imshow(Image.fromarray(mnist_dataset_train.train_data[i].numpy(), mode='L'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader of the training set\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset_train,batch_size=16, shuffle=True)\n",
    "# loader of the validation set\n",
    "val_loader = torch.utils.data.DataLoader(mnist_dataset_val,batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[0/60000 (0%)]\tBatch loss: 0.149664\n",
      "[16000/60000 (27%)]\tBatch loss: 0.005944\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000788\n",
      "[48000/60000 (80%)]\tBatch loss: 0.004036\n",
      "Training: Epoch average loss 0.012215\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.010640 \n",
      "\n",
      "Epoch 2\n",
      "[0/60000 (0%)]\tBatch loss: 0.000170\n",
      "[16000/60000 (27%)]\tBatch loss: 0.004268\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000281\n",
      "[48000/60000 (80%)]\tBatch loss: 0.001623\n",
      "Training: Epoch average loss 0.005902\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.004933 \n",
      "\n",
      "Epoch 3\n",
      "[0/60000 (0%)]\tBatch loss: 0.000315\n",
      "[16000/60000 (27%)]\tBatch loss: 0.003874\n",
      "[32000/60000 (53%)]\tBatch loss: 0.019031\n",
      "[48000/60000 (80%)]\tBatch loss: 0.004662\n",
      "Training: Epoch average loss 0.005086\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.010060 \n",
      "\n",
      "Epoch 4\n",
      "[0/60000 (0%)]\tBatch loss: 0.002215\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000539\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000756\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000550\n",
      "Training: Epoch average loss 0.004650\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.006588 \n",
      "\n",
      "Epoch 5\n",
      "[0/60000 (0%)]\tBatch loss: 0.002461\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000103\n",
      "[32000/60000 (53%)]\tBatch loss: 0.019579\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000289\n",
      "Training: Epoch average loss 0.004252\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.005448 \n",
      "\n",
      "Epoch 6\n",
      "[0/60000 (0%)]\tBatch loss: 0.000120\n",
      "[16000/60000 (27%)]\tBatch loss: 0.009450\n",
      "[32000/60000 (53%)]\tBatch loss: 0.006470\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000353\n",
      "Training: Epoch average loss 0.004079\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.006656 \n",
      "\n",
      "Epoch 7\n",
      "[0/60000 (0%)]\tBatch loss: 0.000065\n",
      "[16000/60000 (27%)]\tBatch loss: 0.014149\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000139\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000325\n",
      "Training: Epoch average loss 0.003845\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.004549 \n",
      "\n",
      "Epoch 8\n",
      "[0/60000 (0%)]\tBatch loss: 0.001066\n",
      "[16000/60000 (27%)]\tBatch loss: 0.001811\n",
      "[32000/60000 (53%)]\tBatch loss: 0.003886\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000376\n",
      "Training: Epoch average loss 0.003798\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.006845 \n",
      "\n",
      "Epoch 9\n",
      "[0/60000 (0%)]\tBatch loss: 0.001410\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000810\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000223\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000461\n",
      "Training: Epoch average loss 0.003709\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.006203 \n",
      "\n",
      "Epoch 10\n",
      "[0/60000 (0%)]\tBatch loss: 0.000433\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000085\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000088\n",
      "[48000/60000 (80%)]\tBatch loss: 0.007026\n",
      "Training: Epoch average loss 0.003493\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.008441 \n",
      "\n",
      "Epoch 11\n",
      "[0/60000 (0%)]\tBatch loss: 0.002725\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000085\n",
      "[32000/60000 (53%)]\tBatch loss: 0.005887\n",
      "[48000/60000 (80%)]\tBatch loss: 0.018543\n",
      "Training: Epoch average loss 0.003525\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.005891 \n",
      "\n",
      "Epoch 12\n",
      "[0/60000 (0%)]\tBatch loss: 0.002930\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000401\n",
      "[32000/60000 (53%)]\tBatch loss: 0.003616\n",
      "[48000/60000 (80%)]\tBatch loss: 0.014394\n",
      "Training: Epoch average loss 0.003481\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.006258 \n",
      "\n",
      "Epoch 13\n",
      "[0/60000 (0%)]\tBatch loss: 0.016403\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000559\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000107\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000305\n",
      "Training: Epoch average loss 0.003336\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.005286 \n",
      "\n",
      "Epoch 14\n",
      "[0/60000 (0%)]\tBatch loss: 0.000014\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000641\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000655\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000537\n",
      "Training: Epoch average loss 0.003352\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.006665 \n",
      "\n",
      "Epoch 15\n",
      "[0/60000 (0%)]\tBatch loss: 0.000172\n",
      "[16000/60000 (27%)]\tBatch loss: 0.004578\n",
      "[32000/60000 (53%)]\tBatch loss: 0.007522\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000151\n",
      "Training: Epoch average loss 0.003257\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.005814 \n",
      "\n",
      "Epoch 16\n",
      "[0/60000 (0%)]\tBatch loss: 0.000834\n",
      "[16000/60000 (27%)]\tBatch loss: 0.001260\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000024\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000070\n",
      "Training: Epoch average loss 0.003178\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.005225 \n",
      "\n",
      "Epoch 17\n",
      "[0/60000 (0%)]\tBatch loss: 0.000570\n",
      "[16000/60000 (27%)]\tBatch loss: 0.001239\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000033\n",
      "[48000/60000 (80%)]\tBatch loss: 0.016983\n",
      "Training: Epoch average loss 0.003096\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.005987 \n",
      "\n",
      "Epoch 18\n",
      "[0/60000 (0%)]\tBatch loss: 0.001354\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000263\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000024\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000204\n",
      "Training: Epoch average loss 0.003126\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.005067 \n",
      "\n",
      "Epoch 19\n",
      "[0/60000 (0%)]\tBatch loss: 0.000081\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000046\n",
      "[32000/60000 (53%)]\tBatch loss: 0.006052\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000371\n",
      "Training: Epoch average loss 0.003098\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.005265 \n",
      "\n",
      "Epoch 20\n",
      "[0/60000 (0%)]\tBatch loss: 0.003719\n",
      "[16000/60000 (27%)]\tBatch loss: 0.000162\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000103\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000143\n",
      "Training: Epoch average loss 0.003043\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.006096 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_no_dropout = MnistNet() # initialize network structure\n",
    "\n",
    "optimizer = optim.SGD(model_no_dropout.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# loss, accuracy = test(model, val_loader)\n",
    "# print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n",
    "\n",
    "loss_history = []\n",
    "accuracy_hostory = []\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "        print('Epoch {}'.format(epoch))\n",
    "        train(model_no_dropout, train_loader, optimizer)\n",
    "        loss, accuracy = test(model_no_dropout, val_loader)\n",
    "        loss_history.append(loss)\n",
    "        accuracy_hostory.append(accuracy)\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.010640297085046768, 0.004933014884591103, 0.010060443729162215, 0.006587996333837509, 0.005448458716273308, 0.006656131148338318, 0.004548512771725655, 0.006845234334468842, 0.006202542781829834, 0.008441054075956345, 0.005890649184584618, 0.006258149445056915, 0.005286131426692009, 0.006665366888046265, 0.005813745409250259, 0.005224801599979401, 0.005987083166837692, 0.005067266523838043, 0.005265174806118012, 0.006095708906650543]\n",
      "[0.9706, 0.9763, 0.9745, 0.978, 0.9785, 0.9804, 0.9814, 0.9809, 0.9827, 0.9802, 0.9836, 0.9794, 0.982, 0.9843, 0.9828, 0.9845, 0.9825, 0.9839, 0.9842, 0.9798]\n"
     ]
    }
   ],
   "source": [
    "print(loss_history)\n",
    "print(accuracy_hostory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader of the training set\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset_train,batch_size=16, shuffle=True)\n",
    "# loader of the validation set\n",
    "val_loader = torch.utils.data.DataLoader(mnist_dataset_val,batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[0/60000 (0%)]\tBatch loss: 0.147084\n",
      "[16000/60000 (27%)]\tBatch loss: 0.010317\n",
      "[32000/60000 (53%)]\tBatch loss: 0.029083\n",
      "[48000/60000 (80%)]\tBatch loss: 0.016208\n",
      "Training: Epoch average loss 0.027389\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.008200 \n",
      "\n",
      "Epoch 2\n",
      "[0/60000 (0%)]\tBatch loss: 0.026261\n",
      "[16000/60000 (27%)]\tBatch loss: 0.027519\n",
      "[32000/60000 (53%)]\tBatch loss: 0.036340\n",
      "[48000/60000 (80%)]\tBatch loss: 0.015500\n",
      "Training: Epoch average loss 0.018925\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.009406 \n",
      "\n",
      "Epoch 3\n",
      "[0/60000 (0%)]\tBatch loss: 0.001093\n",
      "[16000/60000 (27%)]\tBatch loss: 0.046925\n",
      "[32000/60000 (53%)]\tBatch loss: 0.006641\n",
      "[48000/60000 (80%)]\tBatch loss: 0.009487\n",
      "Training: Epoch average loss 0.016954\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.007677 \n",
      "\n",
      "Epoch 4\n",
      "[0/60000 (0%)]\tBatch loss: 0.046682\n",
      "[16000/60000 (27%)]\tBatch loss: 0.014979\n",
      "[32000/60000 (53%)]\tBatch loss: 0.010446\n",
      "[48000/60000 (80%)]\tBatch loss: 0.001663\n",
      "Training: Epoch average loss 0.016386\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.006760 \n",
      "\n",
      "Epoch 5\n",
      "[0/60000 (0%)]\tBatch loss: 0.003492\n",
      "[16000/60000 (27%)]\tBatch loss: 0.002135\n",
      "[32000/60000 (53%)]\tBatch loss: 0.007818\n",
      "[48000/60000 (80%)]\tBatch loss: 0.007971\n",
      "Training: Epoch average loss 0.015744\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.006979 \n",
      "\n",
      "Epoch 6\n",
      "[0/60000 (0%)]\tBatch loss: 0.004384\n",
      "[16000/60000 (27%)]\tBatch loss: 0.003094\n",
      "[32000/60000 (53%)]\tBatch loss: 0.020091\n",
      "[48000/60000 (80%)]\tBatch loss: 0.009015\n",
      "Training: Epoch average loss 0.015567\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.008307 \n",
      "\n",
      "Epoch 7\n",
      "[0/60000 (0%)]\tBatch loss: 0.014142\n",
      "[16000/60000 (27%)]\tBatch loss: 0.011952\n",
      "[32000/60000 (53%)]\tBatch loss: 0.019837\n",
      "[48000/60000 (80%)]\tBatch loss: 0.018341\n",
      "Training: Epoch average loss 0.015286\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.007122 \n",
      "\n",
      "Epoch 8\n",
      "[0/60000 (0%)]\tBatch loss: 0.021360\n",
      "[16000/60000 (27%)]\tBatch loss: 0.002354\n",
      "[32000/60000 (53%)]\tBatch loss: 0.001631\n",
      "[48000/60000 (80%)]\tBatch loss: 0.000724\n",
      "Training: Epoch average loss 0.014779\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.007487 \n",
      "\n",
      "Epoch 9\n",
      "[0/60000 (0%)]\tBatch loss: 0.004133\n",
      "[16000/60000 (27%)]\tBatch loss: 0.047220\n",
      "[32000/60000 (53%)]\tBatch loss: 0.016369\n",
      "[48000/60000 (80%)]\tBatch loss: 0.005518\n",
      "Training: Epoch average loss 0.014632\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.007595 \n",
      "\n",
      "Epoch 10\n",
      "[0/60000 (0%)]\tBatch loss: 0.026911\n",
      "[16000/60000 (27%)]\tBatch loss: 0.011652\n",
      "[32000/60000 (53%)]\tBatch loss: 0.001388\n",
      "[48000/60000 (80%)]\tBatch loss: 0.021868\n",
      "Training: Epoch average loss 0.014355\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.006733 \n",
      "\n",
      "Epoch 11\n",
      "[0/60000 (0%)]\tBatch loss: 0.015624\n",
      "[16000/60000 (27%)]\tBatch loss: 0.027787\n",
      "[32000/60000 (53%)]\tBatch loss: 0.015716\n",
      "[48000/60000 (80%)]\tBatch loss: 0.005883\n",
      "Training: Epoch average loss 0.014481\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.006180 \n",
      "\n",
      "Epoch 12\n",
      "[0/60000 (0%)]\tBatch loss: 0.014470\n",
      "[16000/60000 (27%)]\tBatch loss: 0.003559\n",
      "[32000/60000 (53%)]\tBatch loss: 0.014265\n",
      "[48000/60000 (80%)]\tBatch loss: 0.003444\n",
      "Training: Epoch average loss 0.014215\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.006091 \n",
      "\n",
      "Epoch 13\n",
      "[0/60000 (0%)]\tBatch loss: 0.012836\n",
      "[16000/60000 (27%)]\tBatch loss: 0.019271\n",
      "[32000/60000 (53%)]\tBatch loss: 0.012907\n",
      "[48000/60000 (80%)]\tBatch loss: 0.027509\n",
      "Training: Epoch average loss 0.014260\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.006402 \n",
      "\n",
      "Epoch 14\n",
      "[0/60000 (0%)]\tBatch loss: 0.021969\n",
      "[16000/60000 (27%)]\tBatch loss: 0.008133\n",
      "[32000/60000 (53%)]\tBatch loss: 0.003191\n",
      "[48000/60000 (80%)]\tBatch loss: 0.013059\n",
      "Training: Epoch average loss 0.014168\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 15\n",
      "[0/60000 (0%)]\tBatch loss: 0.005178\n",
      "[16000/60000 (27%)]\tBatch loss: 0.013837\n",
      "[32000/60000 (53%)]\tBatch loss: 0.021539\n",
      "[48000/60000 (80%)]\tBatch loss: 0.062801\n",
      "Training: Epoch average loss 0.014225\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.005604 \n",
      "\n",
      "Epoch 16\n",
      "[0/60000 (0%)]\tBatch loss: 0.014129\n",
      "[16000/60000 (27%)]\tBatch loss: 0.011069\n",
      "[32000/60000 (53%)]\tBatch loss: 0.000189\n",
      "[48000/60000 (80%)]\tBatch loss: 0.003212\n",
      "Training: Epoch average loss 0.013958\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.005346 \n",
      "\n",
      "Epoch 17\n",
      "[0/60000 (0%)]\tBatch loss: 0.017205\n",
      "[16000/60000 (27%)]\tBatch loss: 0.008604\n",
      "[32000/60000 (53%)]\tBatch loss: 0.020792\n",
      "[48000/60000 (80%)]\tBatch loss: 0.017554\n",
      "Training: Epoch average loss 0.013852\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.006433 \n",
      "\n",
      "Epoch 18\n",
      "[0/60000 (0%)]\tBatch loss: 0.013786\n",
      "[16000/60000 (27%)]\tBatch loss: 0.018082\n",
      "[32000/60000 (53%)]\tBatch loss: 0.002700\n",
      "[48000/60000 (80%)]\tBatch loss: 0.017722\n",
      "Training: Epoch average loss 0.013576\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.006389 \n",
      "\n",
      "Epoch 19\n",
      "[0/60000 (0%)]\tBatch loss: 0.010165\n",
      "[16000/60000 (27%)]\tBatch loss: 0.014364\n",
      "[32000/60000 (53%)]\tBatch loss: 0.005853\n",
      "[48000/60000 (80%)]\tBatch loss: 0.004199\n",
      "Training: Epoch average loss 0.013914\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.005818 \n",
      "\n",
      "Epoch 20\n",
      "[0/60000 (0%)]\tBatch loss: 0.009173\n",
      "[16000/60000 (27%)]\tBatch loss: 0.003632\n",
      "[32000/60000 (53%)]\tBatch loss: 0.002507\n",
      "[48000/60000 (80%)]\tBatch loss: 0.011374\n",
      "Training: Epoch average loss 0.013901\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.005226 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_dropout = MnistNet(dropout=True) # initialize network structure\n",
    "\n",
    "optimizer = optim.SGD(model_dropout.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "loss_history = []\n",
    "accuracy_hostory = []\n",
    "# loss, accuracy = test(model, val_loader)\n",
    "# print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "        print('Epoch {}'.format(epoch))\n",
    "        train(model_dropout, train_loader, optimizer)\n",
    "        loss, accuracy = test(model_dropout, val_loader)\n",
    "        loss_history.append(loss)\n",
    "        accuracy_hostory.append(accuracy)\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02168576717376709, 0.016968713700771333, 0.01752651482820511, 0.016523778438568115, 0.018740127980709075, 0.017744752764701843, 0.013278430700302124, 0.014398089051246643, 0.014035139977931977]\n",
      "[0.9283, 0.9454, 0.9467, 0.947, 0.9401, 0.9501, 0.9478, 0.9499, 0.9499]\n"
     ]
    }
   ],
   "source": [
    "print(loss_history)\n",
    "print(accuracy_hostory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
