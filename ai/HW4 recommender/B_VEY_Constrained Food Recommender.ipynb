{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Food Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement both Content Based and Collaborative Filtering Recommenders and backtracking search (or local search) on your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% finished homework should contain EDA, Item and User profiles generation, Content-Based Recommender, Collaborative Filtering Recommender, and soluton to CSP problem of assigning recommendations to brekfast, lunch and dinner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will work with subset of [Academic Yelp Dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset) containing list of restaurants in **yelp_business.csv** and reviews of the users in **yelp_reviews.parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_yelp_business = pd.read_csv(\"yelp_business.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "df_yelp_reviews = pd.read_parquet(\"yelp_reviews.parquet\")\n",
    "\n",
    "# Leave only users with at least 3 reviews\n",
    "users_count = df_yelp_reviews.groupby(\"user_id\").count()[[\"business_id\"]] \n",
    "users_to_use = users_count[users_count[\"business_id\"] > 2]\n",
    "df_yelp_reviews = df_yelp_reviews[df_yelp_reviews[\"user_id\"].isin(users_to_use.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will explore the data to find out what is the distribution of business categories, hours, places, user reviews, etc.\n",
    "\n",
    "This step is needed to proceed later with item and user profiling and to clean your data if there are duplicates (e.g. duplicated reviews, the same businesses under different ids, categories tags which are highly correlated) or some artifacts not related to the main task.\n",
    "\n",
    "(5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Ethnic Food, Food Trucks, Specialty Food, Impo...\n",
       "1            Food, Restaurants, Grocery, Middle Eastern\n",
       "2          Japanese, Fast Food, Food Court, Restaurants\n",
       "3      Food, Pretzels, Bakeries, Fast Food, Restaurants\n",
       "4                       Mexican, Restaurants, Fast Food\n",
       "                            ...                        \n",
       "95                         Restaurants, Spanish, French\n",
       "96    Fast Food, Chicken Shop, Restaurants, Chicken ...\n",
       "97                    Restaurants, Steakhouses, Seafood\n",
       "98    Coffee & Tea, Food, Juice Bars & Smoothies, De...\n",
       "99                                   Delis, Restaurants\n",
       "Name: categories, Length: 100, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_business.categories.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IS4cv902ykd8wj1TR0N3-A</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-14 21:56:57</td>\n",
       "      <td>0</td>\n",
       "      <td>6TdNDKywdbjoTkizeMce8A</td>\n",
       "      <td>4</td>\n",
       "      <td>happy day finally canes near casa yes others g...</td>\n",
       "      <td>0</td>\n",
       "      <td>UgMW8bLE0QMJDCkQ1Ax5Mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pthe4qk5xh4n-ef-9bvMSg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11-05 23:11:05</td>\n",
       "      <td>0</td>\n",
       "      <td>ZayJ1zWyWgY9S_TRLT_y9Q</td>\n",
       "      <td>5</td>\n",
       "      <td>really good place simple decor amazing food gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>aq_ZxGHiri48TUXJlpRkCQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ws8V970-mQt2X9CwCuT5zw</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-10-13 04:16:41</td>\n",
       "      <td>0</td>\n",
       "      <td>z4BCgTkfNtCu4XY5Lp97ww</td>\n",
       "      <td>4</td>\n",
       "      <td>twice nice laid back tried weekend southern me...</td>\n",
       "      <td>3</td>\n",
       "      <td>jOERvhmK6_lo_XGUBPws_w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>d4qwVw4PcN-_2mK2o1Ro1g</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-02-02 06:28:00</td>\n",
       "      <td>0</td>\n",
       "      <td>bVTjZgRNq8ToxzvtiVrqMA</td>\n",
       "      <td>1</td>\n",
       "      <td>10pm super bowl sunday already closed weak won...</td>\n",
       "      <td>0</td>\n",
       "      <td>2hRe26HSCAWbFRn5WChK-Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9Jo1pu0y2zU6ktiwQm6gNA</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-12-04 03:15:21</td>\n",
       "      <td>19</td>\n",
       "      <td>sgTnHfeaEvyOoWX4TCgkuQ</td>\n",
       "      <td>4</td>\n",
       "      <td>coconut fish cafe fantastic five stars fish ca...</td>\n",
       "      <td>24</td>\n",
       "      <td>A0j21z2Q1HGic7jW6e9h7A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               business_id  cool                date  funny  \\\n",
       "4   IS4cv902ykd8wj1TR0N3-A     0 2017-01-14 21:56:57      0   \n",
       "6   Pthe4qk5xh4n-ef-9bvMSg     0 2015-11-05 23:11:05      0   \n",
       "9   Ws8V970-mQt2X9CwCuT5zw     1 2009-10-13 04:16:41      0   \n",
       "16  d4qwVw4PcN-_2mK2o1Ro1g     0 2015-02-02 06:28:00      0   \n",
       "22  9Jo1pu0y2zU6ktiwQm6gNA    20 2016-12-04 03:15:21     19   \n",
       "\n",
       "                 review_id  stars  \\\n",
       "4   6TdNDKywdbjoTkizeMce8A      4   \n",
       "6   ZayJ1zWyWgY9S_TRLT_y9Q      5   \n",
       "9   z4BCgTkfNtCu4XY5Lp97ww      4   \n",
       "16  bVTjZgRNq8ToxzvtiVrqMA      1   \n",
       "22  sgTnHfeaEvyOoWX4TCgkuQ      4   \n",
       "\n",
       "                                                 text  useful  \\\n",
       "4   happy day finally canes near casa yes others g...       0   \n",
       "6   really good place simple decor amazing food gr...       1   \n",
       "9   twice nice laid back tried weekend southern me...       3   \n",
       "16  10pm super bowl sunday already closed weak won...       0   \n",
       "22  coconut fish cafe fantastic five stars fish ca...      24   \n",
       "\n",
       "                   user_id  \n",
       "4   UgMW8bLE0QMJDCkQ1Ax5Mg  \n",
       "6   aq_ZxGHiri48TUXJlpRkCQ  \n",
       "9   jOERvhmK6_lo_XGUBPws_w  \n",
       "16  2hRe26HSCAWbFRn5WChK-Q  \n",
       "22  A0j21z2Q1HGic7jW6e9h7A  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp_reviews = df_yelp_reviews.drop_duplicates()\n",
    "df_yelp_business = df_yelp_business.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude = df_yelp_business.longitude.copy()\n",
    "longitude = longitude.astype(str)\n",
    "latitude = df_yelp_business.latitude.copy()\n",
    "latitude = latitude.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        40.11044570000001\n",
       "1                35.194894\n",
       "2               43.8204923\n",
       "3       33.602821999999996\n",
       "4       36.099737899999994\n",
       "               ...        \n",
       "5743         51.0730357311\n",
       "5744            43.7310425\n",
       "5745            43.7053513\n",
       "5746     41.48468629999999\n",
       "5747    41.396168599999996\n",
       "Name: latitude, Length: 5748, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp_business['full_name'] = df_yelp_business['name'] + \":\" + longitude + \":\" + latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_business.full_name.value_counts().max() # So there are no exactly same restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp_business = df_yelp_business.drop(columns=['full_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all you should process user reviews to get the utility matrix containing ratings for users and businesses. There will be a lot of 0 in this matrix and it is better to store such matrices in the specialized data structure for sparce matrices. However, your working dataset is relatively small and we can use simple **pd.DataFrame** to proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_utility_matrix(reviews: pd.DataFrame, business: pd.DataFrame) -> pd.DataFrame:\n",
    "    business_ids = business[\"business_id\"].unique()\n",
    "    users = reviews[\"user_id\"].unique()\n",
    "\n",
    "    ut_matrix = pd.DataFrame(0, columns=business_ids, index=users)\n",
    "    for _, review in reviews.iterrows():\n",
    "        ut_matrix.loc[review[\"user_id\"], review[\"business_id\"]] = review[\"stars\"]\n",
    "    \n",
    "    min_star = 1\n",
    "    max_star = 5\n",
    "    ut_matrix[ut_matrix == 3] = 3.000001 # It's needed for at_least_one_visited_place_constraint, and as\n",
    "    # there are not many such values, it will not affect main code\n",
    "    ut_matrix[ut_matrix == 0] = 3 # It's average review, and when we normalize it transforms to 0\n",
    "    ut_matrix = 2 * (ut_matrix - min_star) / (max_star - min_star) - 1\n",
    "    \n",
    "    return ut_matrix\n",
    "\n",
    "df_utility_matrix = create_utility_matrix(df_yelp_reviews, df_yelp_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pQeaRpvuhoEqudo3uymHIQ    0.0\n",
       " CsLQLiRoafpJPJSkNX2h5Q    0.0\n",
       " lu7vtrp_bE9PnxWfA8g4Pg    0.0\n",
       " vjTVxnsQEZ34XjYNS-XUpA    0.0\n",
       " fnZrZlqW1Z8iWgTVDfv_MA   -1.0\n",
       "                          ... \n",
       " gp_bu7Ah81qaBY3M0Leffw    0.0\n",
       " PUKOr5bEI87TVHjwijT1xw    0.0\n",
       " zV38gkkEeJ4cVRlSWWQTfQ   -0.5\n",
       " H1j34TgbrVZkxeww9xlJTw   -0.5\n",
       " F8M0IukXQqR50IRyocRQbg   -1.0\n",
       " Length: 5748, dtype: float64,\n",
       " pQeaRpvuhoEqudo3uymHIQ    0.000000e+00\n",
       " CsLQLiRoafpJPJSkNX2h5Q    5.000000e-07\n",
       " lu7vtrp_bE9PnxWfA8g4Pg    5.000000e-01\n",
       " vjTVxnsQEZ34XjYNS-XUpA    1.000000e+00\n",
       " fnZrZlqW1Z8iWgTVDfv_MA    1.000000e+00\n",
       "                               ...     \n",
       " gp_bu7Ah81qaBY3M0Leffw    5.000000e-01\n",
       " PUKOr5bEI87TVHjwijT1xw    0.000000e+00\n",
       " zV38gkkEeJ4cVRlSWWQTfQ    1.000000e+00\n",
       " H1j34TgbrVZkxeww9xlJTw    1.000000e+00\n",
       " F8M0IukXQqR50IRyocRQbg    5.000000e-07\n",
       " Length: 5748, dtype: float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_utility_matrix.min(),df_utility_matrix.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convenience stores             int64\n",
       "tuscan                         int64\n",
       "team building activities       int64\n",
       "active life                    int64\n",
       "cosmetics & beauty supply      int64\n",
       "                              ...   \n",
       "office cleaning                int64\n",
       "pet stores                     int64\n",
       "art galleries                  int64\n",
       "soba                           int64\n",
       "stars                        float64\n",
       "Length: 412, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "def build_business_profiles(business: pd.DataFrame) -> pd.DataFrame:\n",
    "    # TODO: Feature engineering (10 points)\n",
    "    business = business.copy()\n",
    "    columns_to_save = ['is_open','review_count','stars']\n",
    "    all_categories = set()\n",
    "    for categories in business.categories:\n",
    "        categories = [x.lower().strip() for x in categories.split(',')]\n",
    "        all_categories = all_categories | set(categories)\n",
    "    for category in all_categories:\n",
    "        business.insert(1,category,0,True)\n",
    "    i = 0\n",
    "    for categories in business.categories:\n",
    "        categories = [x.lower().strip() for x in categories.split(',')]\n",
    "        for category in set(categories):\n",
    "            business.at[i,category] = 1\n",
    "        i+=1\n",
    "    # I will not work with attributes, as they contain to many information, which could lead to overfit,\n",
    "    # but I decide to work with all categories, as it's hard for our model to work with text, I transform\n",
    "    # all data using one-hot encoding, \n",
    "    # also I decide to remove all data contain coordinates(address, lattitude, longitude, state and postcode)\n",
    "    # I also remove name column, as it contain more than 4500 different names,\n",
    "    # and in the beggining I decide that different restaurant is restaurant with different coordinates, so we don't\n",
    "    # need to use this information\n",
    "    business = business.drop(columns=['categories','attributes','latitude','longitude',\n",
    "                                      'state','postal_code','city','hours','is_open','review_count','name','address'])\n",
    "    business = business.set_index([\"business_id\"])\n",
    "    return  business\n",
    "\n",
    "df_business_profiles = build_business_profiles(df_yelp_business)\n",
    "df_business_profiles.sum()\n",
    "df_business_profiles.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_user_profiles(utility_matrix: pd.DataFrame, \n",
    "                                   business_profiles: pd.DataFrame) -> pd.DataFrame:\n",
    "    # I will use something like TFIDF encoding, \n",
    "    # where I will add some value divided by total number of liked restaurant for specific user,\n",
    "    # so it will be in range (-1,1) but then \n",
    "    # I will use normalization to get value from (0,1) as in business_profiles,\n",
    "    # than I will also set stars to 5, as every user liked to visit restaurant with bigger review\n",
    "    df = pd.DataFrame(0, index=utility_matrix.index, columns=business_profiles.columns,dtype=float)\n",
    "    i = 0\n",
    "    for user in utility_matrix.index:\n",
    "        reviews = utility_matrix.loc[user]\n",
    "        \n",
    "        user_reviews = reviews[reviews != 0]\n",
    "        visited_businesses = df_business_profiles[df_business_profiles.index.isin(list(user_reviews.index))]\n",
    "        weighted_reviews = visited_businesses.mul(user_reviews, axis=0)\n",
    "        weighted_reviews = weighted_reviews/len(user_reviews)\n",
    "        weighted_reviews = weighted_reviews.sum(axis=0)\n",
    "        df.loc[user] = weighted_reviews\n",
    "        df.loc[user] = (df.loc[user] + 1)/2 \n",
    "        df.at[user,'stars'] = 5\n",
    "        i+=1\n",
    "    return df\n",
    "df_user_profiles = build_user_profiles(df_utility_matrix, df_business_profiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_content_ratings(user_profiles: pd.DataFrame, business_profiles: pd.DataFrame) -> pd.DataFrame:\n",
    "    # TODO: Distance based rating prediction (5 points)\n",
    "    # TODO: Pointwise/Pairwase training based prediction (optional for 10 extra points)\n",
    "    df = pd.DataFrame(0, index=user_profiles.index, columns=business_profiles.index,dtype=float)\n",
    "    for user in user_profiles.index:\n",
    "        user_df = user_profiles.loc[user]\n",
    "        bussiness_df = business_profiles.sub(user_df,axis=1)\n",
    "        business_df = bussiness_df.abs().sum(axis=1) #Manhetten distance\n",
    "        # Normalize our values from 0 to 1\n",
    "        mini = business_df.iloc[business_df.argmin()]\n",
    "        maxi = business_df.iloc[business_df.argmax()] - mini\n",
    "        business_df-=mini\n",
    "        business_df/=maxi\n",
    "        business_df = 1 - business_df\n",
    "\n",
    "        df.loc[user] = business_df\n",
    "    return df\n",
    "df_content_predictions = predict_content_ratings(df_user_profiles, df_business_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "def calculate_similarity(utility_matrix: pd.DataFrame) -> pd.DataFrame:\n",
    "    sim_matrix = pd.DataFrame(0,columns = utility_matrix.index,index=utility_matrix.index,dtype=int)\n",
    "    x = 0\n",
    "    t = time.time()\n",
    "    sets = [set() for _ in range(utility_matrix.shape[0])]\n",
    "    for i in utility_matrix.iterrows():\n",
    "        ser = i[1]\n",
    "        ar = np.array(ser)\n",
    "        ar = ar.nonzero()\n",
    "        sets[x] = set(copy.copy(list(ar[0])))\n",
    "        x+=1\n",
    "    n = utility_matrix.shape[0]\n",
    "    t = time.time()\n",
    "    for i in range(n):\n",
    "        for j in range(i,n):\n",
    "            k = list(sets[i] & sets[j])\n",
    "            if k:\n",
    "                k = (utility_matrix.iloc[i].iloc[k] == utility_matrix.iloc[j].iloc[k]).sum() \n",
    "                if k:\n",
    "                    fst = utility_matrix.iloc[i].name\n",
    "                    scd = utility_matrix.iloc[j].name\n",
    "                    sim_matrix.at[fst,scd] = k\n",
    "                    sim_matrix.at[scd,fst] = k\n",
    "    return sim_matrix\n",
    "sim_matrix = calculate_similarity(df_utility_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_collaborative_ratings(utility_matrix: pd.DataFrame) -> pd.DataFrame:\n",
    "    # TODO: User-item collaborative filtering based rating prediction (15 points)\n",
    "    # TODO: UV-decomposition based rating prediction (optional for 10 extra points)\n",
    "    df = pd.DataFrame(0, index=utility_matrix.index, columns=utility_matrix.columns,dtype=float)\n",
    "    for i in utility_matrix.iterrows():\n",
    "        best = list(sim_matrix.loc[i[0]].nlargest(6).index)[1:]\n",
    "        top_5 = utility_matrix.loc[best].sum(axis=0) / 5\n",
    "        df.loc[i[0]] = top_5\n",
    "    return df\n",
    "df_collaborative_predictions = predict_collaborative_ratings(df_utility_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content-based approach RMSE 0.5609759146392509\n",
      "content-based approach MAP: 0.5307191173643095\n",
      "content-based approach Coverage: 0.997968092472773\n",
      "content-based approach Personalization: 0\n",
      "content-based approach Intra-list similarity: 0\n",
      "collaborative-filtering approach RMSE 0.019181355371861986\n",
      "collaborative-filtering approach MAP: 0.0012088974553572172\n",
      "collaborative-filtering approach Coverage: 0.009629975945889546\n",
      "collaborative-filtering approach Personalization: 0\n",
      "collaborative-filtering approach Intra-list similarity: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def score_model(utility_matrix: pd.DataFrame, predicted_utility_matrix: pd.DataFrame, model_name=\"model_0\"):\n",
    "    # TODO: Implement these by hand (each metric 1 point)\n",
    "    rmse_score = np.sqrt(((predicted_utility_matrix - utility_matrix) *\n",
    "                              (predicted_utility_matrix - utility_matrix)).sum().sum()\n",
    "                                 /utility_matrix.shape[0] / utility_matrix.shape[1])\n",
    "    map_score = np.mean(np.mean(np.abs((utility_matrix - predicted_utility_matrix))))\n",
    "    coverage_score = 1 - np.mean(np.mean(utility_matrix == predicted_utility_matrix))\n",
    "    personalization_score = 0\n",
    "    intra_list_similarity_score = 0\n",
    "    \n",
    "    print(\"{} RMSE {}\".format(model_name, rmse_score))\n",
    "    print(\"{} MAP: {}\".format(model_name, map_score))\n",
    "    print(\"{} Coverage: {}\".format(model_name, coverage_score))\n",
    "    print(\"{} Personalization: {}\".format(model_name, personalization_score))\n",
    "    print(\"{} Intra-list similarity: {}\".format(model_name, intra_list_similarity_score))    \n",
    "\n",
    "score_model(df_content_predictions, df_utility_matrix, \"content-based approach\")\n",
    "score_model(df_collaborative_predictions, df_utility_matrix, \"collaborative-filtering approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint Satisfaction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can work with the task of planing breakfast, lunch and dinner for particular user as Constraint Satisfaction Problem with\n",
    "\n",
    "**Domain**: {all_businesses}\n",
    "\n",
    "**Variables**: {breakfast, lunch, dinner}\n",
    "\n",
    "**Constraints**: {constrainst regarding individual variable, or several variables at once}\n",
    "\n",
    "We also have predicted ratings for every business and want to have personalized plan of restaurants. So we won't only satisfy our constraints, but also would like to get the maximum cumulative rating.\n",
    "\n",
    "Take a look on prepared constraints and finish empty constraints in similar way (some of these constraints may require analytics on business data. e.g. to finish **has_coffee_constraint** you may need to determine all the categories which may include good coffee in their menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "def is_vegetarian_constraint(business_id):\n",
    "    try:\n",
    "        return \"vegetarian\" in df_yelp_business[df_yelp_business[\"business_id\"] == business_id].categories.values[0].lower()\n",
    "    except:\n",
    "        return False\n",
    "def has_coffee_constraint(business_id):\n",
    "    # TODO: implement this constraint (1 point)\n",
    "    return \"coffee\" in df_yelp_business[df_yelp_business[\"business_id\"] == business_id].categories.values[0].lower()\n",
    "\n",
    "\n",
    "def has_alcohol_constraint(business_id):\n",
    "    try: \n",
    "        return \"alcohol\" in df_yelp_business[df_yelp_business[\"business_id\"] == business_id].attributes.values[0].lower()\n",
    "    except:\n",
    "        return False\n",
    "def is_open_constraint(business_id):\n",
    "    # TODO: implement this constraint (1 point)\n",
    "    try:\n",
    "        return df_yelp_business[df_yelp_business[\"business_id\"] == business_id].is_open.values[0] == 1\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def check_hour(time,time_str):\n",
    "    time = int(time[:2]) * 60 + int(time[3:])\n",
    "    time1 = int(time_str[:2]) * 60 + int(time_str[3:5])\n",
    "    time2 = int(time_str[6:8]) * 60 + int(time_str[9:])\n",
    "    return time1 < time < time2\n",
    "\n",
    "def transform_weekday(weekday:str):\n",
    "    return weekday[0].upper() + weekday[1:].lower()\n",
    "\n",
    "def is_open_at_date_at_time_meta_constraint(weekday, time, business_id):\n",
    "    # TODO: implement this constraint (1 point)\n",
    "    weekday = transform_weekday(weekday)\n",
    "    try:\n",
    "        return is_open_constraint(business_id) and \\\n",
    "            check_hour(time,  ast.literal_eval(df_yelp_business[df_yelp_business[\"business_id\"] == business_id].hours.values[0])[weekday])\n",
    "    except:\n",
    "        return False # Restaurant don't work at that time, or has Nan in hours\n",
    "\n",
    "    \n",
    "def is_open_at_monday_at_10am_constraint(business_id):\n",
    "    return is_open_at_date_at_time_meta_constraint(\"monday\", \"10:00\", business_id)\n",
    "\n",
    "def all_are_different_constraint(state):\n",
    "    for time in [\"breakfast\", \"dinner\", \"lunch\"]:\n",
    "        for _t in [\"breakfast\", \"dinner\", \"lunch\"]:\n",
    "            if time == _t: continue\n",
    "            business_categories = set(df_yelp_business[df_yelp_business[\"business_id\"] == state[time][\"business_id\"]].categories.values[0].split(\",\"))\n",
    "            _business_categories = set(df_yelp_business[df_yelp_business[\"business_id\"] == state[_t][\"business_id\"]].categories.values[0].split(\",\"))\n",
    "            if len(business_categories.intersection(_business_categories)) > \\\n",
    "                    len(business_categories.union(_business_categories)) // 2:\n",
    "                return False\n",
    "    return True\n",
    "def all_are_in_the_same_city_constraint(state):\n",
    "    cities = set()\n",
    "    for time in [\"breakfast\", \"dinner\", \"lunch\"]:\n",
    "        cities = cities | set(df_yelp_business[df_yelp_business[\"business_id\"] == state[time][\"business_id\"]].city)\n",
    "    return len(cities) < 2 # Zero or one\n",
    "def all_are_in_the_same_region_meta_constraint(coordinates, threshold, state):\n",
    "    # TODO: implement this constraint (1 point). Hint: use haversine distance https://pypi.org/project/haversine/\n",
    "    now = True\n",
    "    from haversine import haversine\n",
    "    for time in [\"breakfast\", \"dinner\", \"lunch\"]:\n",
    "            lat = df_yelp_business[df_yelp_business[\"business_id\"] == state[time][\"business_id\"]].latitude.values[0]\n",
    "            lon = df_yelp_business[df_yelp_business[\"business_id\"] == state[time][\"business_id\"]].longitude.values[0]\n",
    "            dst = haversine((float(lat),float(lon)),(coordinates['lat'],coordinates['lon']))\n",
    "            now = now & (dst < threshold)\n",
    "    return now\n",
    "def all_are_in_test_region(state):\n",
    "    return all_are_in_the_same_region_meta_constraint({\"lat\": 40.110446, \"lon\": -115.301568}, 600, state)\n",
    "\n",
    "def at_least_one_visited_place_constraint(state):\n",
    "    # TODO: implement this constraint (2 points)\n",
    "    # Make this constraint give more reward for more than one familiar place\n",
    "    vis = 0\n",
    "    now = 1/2\n",
    "    for time in [\"breakfast\", \"dinner\", \"lunch\"]:\n",
    "        if (df_utility_matrix.loc[state[\"user_id\"]][state[time][\"business_id\"]] != 0):\n",
    "            vis += now\n",
    "            now/=2\n",
    "    return vis\n",
    "\n",
    "def at_least_one_has_coffee_constraint(state):\n",
    "    # TODO: implement this constraint (2 points)\n",
    "    # Make this constraint give more reward for more than one place with coffee\n",
    "    coffee = 0\n",
    "    now = 1/2\n",
    "    for time in [\"breakfast\", \"dinner\", \"lunch\"]:\n",
    "        if has_coffee_constraint(state[time][\"business_id\"]): # So now every time we find coffe our value will rise by one\n",
    "            coffee += now\n",
    "            now/=2\n",
    "\n",
    "    return coffee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 'ZKrS8mZ23hBXTptp1P_qYA', 'breakfast': {'business_id': '1wAIVR71cLfupzNsk13Ryg', 'predicted_rating': 0.2}, 'lunch': {'business_id': 'eoJfl5vG7X87QhcKb0nt5Q', 'predicted_rating': 0.4}, 'dinner': {'business_id': 'zzSYBWuv_fXGtSgsO-6_1g', 'predicted_rating': 0.4}}\n",
      "0.016000000000000004\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "random.seed(42)\n",
    "inspected_user = random.choice(df_yelp_reviews[\"user_id\"].unique())\n",
    "\n",
    "all_constraints = {\n",
    "    \"breakfast\": [has_coffee_constraint, is_open_constraint],\n",
    "    \"lunch\": [is_open_constraint],\n",
    "    \"dinner\": [is_vegetarian_constraint, has_alcohol_constraint, is_open_constraint],\n",
    "    \"state\": [at_least_one_has_coffee_constraint]\n",
    "}\n",
    "\n",
    "def goal_test(state: dict, constraints: dict):\n",
    "    cumulative_rating = state[\"breakfast\"][\"predicted_rating\"]*state[\"lunch\"][\"predicted_rating\"]*\\\n",
    "                        state[\"dinner\"][\"predicted_rating\"]\n",
    "    for k in constraints.keys():\n",
    "        if k == \"state\":\n",
    "            for c in constraints[k]:\n",
    "                cumulative_rating *= c(state)\n",
    "        else:\n",
    "            for c in constraints[k]:\n",
    "                cumulative_rating *= c(state[k][\"business_id\"])\n",
    "    return cumulative_rating\n",
    "\n",
    "def init_filtering(domains,constraints,ratings):\n",
    "    to_return = {\n",
    "        'breakfast':[],\n",
    "        'lunch':[],\n",
    "        'dinner':[]\n",
    "    }\n",
    "    for time in to_return:\n",
    "        for x in domains:\n",
    "            correct = True\n",
    "            for c in constraints[time]:\n",
    "                if not c(x):\n",
    "                    correct = False\n",
    "                    break\n",
    "            \n",
    "            if correct and ratings[x]:                    # This value should have rating more than 0\n",
    "                to_return[time].append(x)\n",
    "                \n",
    "    # Now ordering I will sort by rating, as it's impossible to predict future values\n",
    "    \n",
    "    for time in to_return:\n",
    "        to_return[time].sort(key = lambda x: ratings[x])\n",
    "    \n",
    "    return to_return\n",
    "best_score = 0\n",
    "best_state = None\n",
    "def backtracking(ubs,state,domain_filter,constraints,now):\n",
    "    # We could use filtering only after we know lunch, as it possible to fit state constraint with only one last value\n",
    "    # To be honest in this case there are no any advantages of using filtering(as we already use init_filtering)\n",
    "    # but as it is need at task I will add it.\n",
    "\n",
    "    for bs in domain_filter[now]:\n",
    "        state[now][\"business_id\"] = bs\n",
    "        state[now]['predicted_rating'] = ubs[bs]\n",
    "        if now == 'breakfast':\n",
    "            backtracking(ubs,state,domain_filter,constraints,'lunch')\n",
    "        if now == 'lunch':\n",
    "            copy_domain_filter = copy.deepcopy(domain_filter)\n",
    "            copy_domain_filter['dinner'] = []\n",
    "            for x in domain_filter['dinner']:\n",
    "                correct = True\n",
    "                state['dinner'][\"business_id\"] = x\n",
    "                state['dinner']['predicted_rating'] = ubs[x]\n",
    "                for c in constraints['state']:\n",
    "                    if not c(state):\n",
    "                        correct = False\n",
    "                if correct:\n",
    "                    copy_domain_filter['dinner'].append(x)\n",
    "                state['dinner']['business_id'] = None\n",
    "                state['dinner']['predicted_rating'] = 0\n",
    "                \n",
    "            backtracking(ubs,state,copy_domain_filter,constraints,'dinner')\n",
    "        if now == 'dinner':\n",
    "            global best_score\n",
    "            global best_state\n",
    "            if goal_test(state,constraints) > best_score:\n",
    "                best_score = goal_test(state,constraints)\n",
    "                best_state = copy.deepcopy(state)\n",
    "        state[now][\"business_id\"] = None\n",
    "        state[now]['predicted_rating'] = 0\n",
    "\n",
    "def prepare_restaurants_plan(user_id: str, user_business_ratings: pd.DataFrame, constraints: dict):\n",
    "    # TODO: assign breakfast, lunch and dinner by solving Constraint Satisfaction Problem \n",
    "    # maximizing total score and satisfying all the constraints (it should work with any configuration of constraints)\n",
    "    # You can implement Backtracking (10) + Filtering (10) + Ordering (5) using goal_test\n",
    "    # OR\n",
    "    # Local Search (10) + Min-Conflicts heuristic (10) + Ordering (5) with modification of goal test to work as Min-Conflicts heuristic\n",
    "    state = {\"user_id\" : user_id,\n",
    "        \"breakfast\": \n",
    "                {\"business_id\": None,\n",
    "                 \"predicted_rating\": 0},\n",
    "            \"lunch\": \n",
    "    {\"business_id\": None,\n",
    "                 \"predicted_rating\": 0},\n",
    "            \"dinner\": {\"business_id\": None,\n",
    "                 \"predicted_rating\": 0}}\n",
    "    domains = user_business_ratings.columns\n",
    "    ratings = user_business_ratings.loc[user_id]\n",
    "    domain_filter = init_filtering(domains,constraints,ratings)\n",
    "    state = backtracking(ratings,state,domain_filter,constraints,'breakfast') # All other constraints already correct\n",
    "    state = best_state\n",
    "    state_v = goal_test(state, constraints)\n",
    "\n",
    "    \n",
    "    return state\n",
    "\n",
    "# TODO: replace df_utility_matrix with your best predictions\n",
    "state = prepare_restaurants_plan(inspected_user, df_collaborative_predictions, all_constraints)\n",
    "print(state)\n",
    "print(goal_test(state,all_constraints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
